{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Phi-3.5-mini model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e36a2f7ef814a87ae6c4ae0e496b93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_path = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "phi_model = transformers.AutoModelForCausalLM.from_pretrained(model_path, torchscript=True)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# Prepare test inputs\n",
    "test_inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(test_inputs.input_ids.shape)\n",
    "print(test_inputs.attention_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "phi_output = phi_model.forward(**test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[19.6329, 19.6842, 23.8997,  ..., 23.4851, 23.4887, 23.4872],\n",
       "         [35.2728, 39.5948, 40.0355,  ..., 34.6642, 34.6659, 34.6639],\n",
       "         [34.8641, 36.0914, 34.7488,  ..., 31.1677, 31.1690, 31.1691],\n",
       "         [34.6998, 35.8825, 37.9439,  ..., 30.6538, 30.6549, 30.6538],\n",
       "         [36.7390, 37.6414, 40.1978,  ..., 31.1590, 31.1605, 31.1616],\n",
       "         [39.4064, 46.0986, 46.8669,  ..., 35.0389, 35.0401, 35.0382]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| past_key_values: torch.Size([1, 32, 4, 96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38.6764, 44.1411, 45.2279,  ..., 35.4041, 35.4046, 35.4034]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_output[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap model\n",
    "\n",
    "class PhiModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids, attention_mask)[0]\n",
    "\n",
    "wrapped_model = PhiModel(phi_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_output = wrapped_model(test_inputs.input_ids, test_inputs.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.zeros((1, 2), dtype=torch.int32)\n",
    "attention_mask = torch.ones((1, 2), dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/modeling_utils.py:4674: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:1090: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.original_max_position_embeddings:\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:208: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  ext_factors = torch.tensor(self.short_factor, dtype=torch.float32, device=x.device)\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:401: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(wrapped_model.eval(), (input_ids, attention_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to CoreML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.4.1+cu121 has not been tested with coremltools. You may run into unexpected errors. Torch 2.3.0 is the most recent version that has been tested.\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "\n",
    "query_length = ct.RangeDim(lower_bound=1, upper_bound=2048, default=1)\n",
    "\n",
    "inputs = [\n",
    "    ct.TensorType(name=\"inputIds\", shape=(1, query_length), dtype=np.int32),\n",
    "    ct.TensorType(name=\"attentionMask\", shape=(1, query_length), dtype=np.int32),\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ct.TensorType(name=\"logits\", dtype=np.float16),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/4529 [00:00<?, ? ops/s]Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:   5%|▍         | 223/4529 [00:00<00:01, 2222.04 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  11%|█         | 506/4529 [00:00<00:01, 2536.87 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  17%|█▋        | 787/4529 [00:00<00:01, 2649.87 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  23%|██▎       | 1052/4529 [00:00<00:01, 1816.85 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  29%|██▉       | 1333/4529 [00:00<00:01, 2089.80 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  36%|███▌      | 1614/4529 [00:00<00:01, 2290.42 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  42%|████▏     | 1896/4529 [00:00<00:01, 2420.79 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  48%|████▊     | 2177/4529 [00:00<00:00, 2525.01 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  54%|█████▍    | 2458/4529 [00:01<00:00, 2592.43 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  61%|██████    | 2743/4529 [00:01<00:00, 2667.37 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  67%|██████▋   | 3031/4529 [00:01<00:00, 2721.77 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  73%|███████▎  | 3318/4529 [00:01<00:00, 2765.02 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  80%|███████▉  | 3607/4529 [00:01<00:00, 2801.10 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  86%|████████▌ | 3890/4529 [00:01<00:00, 1916.46 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops:  92%|█████████▏| 4170/4529 [00:01<00:00, 2115.18 ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 4528/4529 [00:01<00:00, 2371.03 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 14.37 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 84/84 [00:11<00:00,  7.31 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 31.54 passes/s]\n"
     ]
    }
   ],
   "source": [
    "fp16_mlmodel = ct.convert(\n",
    "    traced_model.eval(),\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    source=\"pytorch\",\n",
    "    minimum_deployment_target=ct.target.iOS18,\n",
    "    compute_precision=ct.precision.FLOAT32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_mlmodel.save(\"phi-3.5-mini-instruct-fp32.mlpackage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latched",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
